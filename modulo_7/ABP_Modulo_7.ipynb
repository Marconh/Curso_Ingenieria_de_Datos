{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ABP Modulo 7**"
      ],
      "metadata": {
        "id": "e2HNIGMFt4qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Lección 1 — Big Data\n",
        "1) 5V’s aplicadas al proyecto\n",
        "\n",
        "Volumen: múltiples miles de registros (y potencialmente millones si crece) provenientes de ventas, navegación, sensores/logs o eventos—requiere almacenamiento y cómputo escalables.\n",
        "\n",
        "Velocidad: necesidad de procesar y responder rápido (batch + streaming) para reportes o detección de eventos casi en tiempo real.\n",
        "\n",
        "Variedad: formatos heterogéneos (CSV/JSON/Parquet) y datos estructurados/no estructurados.\n",
        "\n",
        "Veracidad: control de calidad (validez, duplicados, valores faltantes) para decisiones confiables. (Está implícito en el objetivo de integrar y documentar bien los datos.)\n",
        "\n",
        "Valor: transformar los datos en insights y en un pipeline reutilizable (consulta, ML y streaming).\n",
        "\n",
        "2) Beneficios del enfoque distribuido frente al local\n",
        "\n",
        "Escalabilidad: divide y conquista; procesa datos masivos en varios nodos en lugar de un solo equipo.\n",
        "\n",
        "Rendimiento: trabajo en memoria y ejecución paralela reducen tiempos en cargas iterativas y ML.\n",
        "\n",
        "Versatilidad en un mismo ecosistema: batch, SQL, ML y streaming bajo una misma plataforma (Spark).\n",
        "\n",
        "Tiempo real/cuasi tiempo real: capacidad de streaming para reaccionar a eventos.\n",
        "\n",
        "3) Mapa de tecnologías clave\n",
        "\n",
        "Ingesta / Orquestación: archivos (CSV/JSON) y/o cola de mensajes (Kafka) si simulas streaming.\n",
        "\n",
        "Almacenamiento:\n",
        "\n",
        "Data Lake en Parquet (carpetas por fecha/partición) para costo/velocidad.\n",
        "\n",
        "Catálogo (metadatos) y scripts de carga/limpieza. (La guía exige integrar CSV/JSON/Parquet).\n",
        "\n",
        "Procesamiento distribuido:\n",
        "\n",
        "Spark SQL/DataFrames para consultas optimizadas.\n",
        "\n",
        "RDDs para transformaciones de bajo nivel cuando aplique.\n",
        "\n",
        "Streaming (Structured Streaming) para flujos en tiempo real/simulados.\n",
        "\n",
        "ML escalable: Spark MLlib (VectorAssembler + modelo supervisado) para entrenamiento y evaluación.\n",
        "\n",
        "Entregables/documentación: scripts, modelo guardado, capturas de streaming y README.\n"
      ],
      "metadata": {
        "id": "49u3p6z4sIwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lección 2: Apache Spark\n",
        "\n",
        "**Cuándo y por qué usar Spark en un entorno Big Data**\n",
        "\n",
        "Spark se usa cuando se necesita procesar grandes volúmenes de datos distribuidos que no caben en un solo computador.\n",
        "\n",
        "Es ideal cuando se requiere alta velocidad, ya que usa procesamiento en memoria mucho más rápido que MapReduce tradicional.\n",
        "\n",
        "Permite trabajar con distintos tipos de datos y fuentes (archivos, bases de datos, streams) dentro de un mismo ecosistema.\n",
        "\n",
        "Se integra con múltiples herramientas del entorno Big Data (Hadoop, Kafka, Cassandra, etc.).\n",
        "\n",
        "Se usa tanto para procesamiento batch como para streaming en tiempo real, además de consultas SQL y machine learning escalable.\n",
        "\n",
        "**Arquitectura general de Spark**\n",
        "\n",
        "Spark funciona bajo un modelo maestro–trabajadores distribuido, compuesto por:\n",
        "\n",
        "Driver:\n",
        "Es el proceso principal. Envía el código de la aplicación, crea el plan de ejecución (DAG) y coordina las tareas entre los ejecutores. Contiene el SparkContext.\n",
        "Se ejecuta una sola vez por aplicación.\n",
        "\n",
        "Executors:\n",
        "Son los procesos que se ejecutan en los nodos del clúster.\n",
        "Reciben tareas del Driver, procesan los datos en paralelo y devuelven los resultados parciales.\n",
        "Cada Executor maneja múltiples tareas concurrentes y usa memoria propia.\n",
        "\n",
        "Cluster Manager:\n",
        "Asigna los recursos del clúster (CPU, RAM) a las aplicaciones de Spark.\n",
        "Puede ser YARN, Mesos, Kubernetes o el Standalone manager de Spark.\n",
        "Permite escalar horizontalmente agregando nodos.\n",
        "\n",
        "\n",
        "Módulos de Spark necesarios para el proyecto **texto en negrita**\n",
        "\n",
        "Para este proyecto usaremos principalmente:\n",
        "\n",
        "Spark SQL / DataFrames → para procesar datos tabulares y ejecutar consultas optimizadas.\n",
        "\n",
        "Spark Structured Streaming → para procesar flujos de datos en tiempo real.\n",
        "\n",
        "Spark MLlib → para construir, entrenar y evaluar modelos de machine learning de forma distribuida."
      ],
      "metadata": {
        "id": "a2zLX928s2L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lección 3 — RDDs en Spark\n",
        "#1) Crear entorno local de Spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Leccion3_RDDs\").getOrCreate()\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "-h3g3Dpoug0y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2) Crear un RDD con datos simulados\n",
        "# Datos simulados (nombre, edad, nota)\n",
        "datos = [\n",
        "    (\"Ana\", 23, 5.8),\n",
        "    (\"Luis\", 30, 6.1),\n",
        "    (\"Carla\", 21, 4.9),\n",
        "    (\"Pedro\", 28, 5.5),\n",
        "    (\"Sofía\", 26, 6.8)\n",
        "]\n",
        "\n",
        "# Crear RDD\n",
        "rdd = sc.parallelize(datos)\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11vya9zru8ts",
        "outputId": "12fb6d87-c1f6-466e-9ac5-4f245bcde974"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ana', 23, 5.8),\n",
              " ('Luis', 30, 6.1),\n",
              " ('Carla', 21, 4.9),\n",
              " ('Pedro', 28, 5.5),\n",
              " ('Sofía', 26, 6.8)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#aplicar transformaciones\n",
        "# Filtrar personas con nota >= 5.5\n",
        "rdd_filtrado = rdd.filter(lambda x: x[2] >= 5.5)\n",
        "\n",
        "# Map: convertir a (nombre, nota*10)\n",
        "rdd_mapeado = rdd_filtrado.map(lambda x: (x[0], x[2]*10))\n",
        "\n",
        "# Crear otro RDD pequeño para usar union\n",
        "extra = sc.parallelize([(\"Nuevo\", 25, 6.0)])\n",
        "rdd_union = rdd.union(extra)\n",
        "\n",
        "# Ordenar por edad\n",
        "rdd_ordenado = rdd_union.sortBy(lambda x: x[1])"
      ],
      "metadata": {
        "id": "0ifiijPTvM5l"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejecutar acciones\n",
        "# Contar registros\n",
        "total = rdd_ordenado.count()\n",
        "\n",
        "# Ver todos\n",
        "todos = rdd_ordenado.collect()\n",
        "\n",
        "# Calcular media de notas\n",
        "mean_nota = rdd.map(lambda x: x[2]).mean()\n",
        "\n",
        "print(\"Total personas:\", total)\n",
        "print(\"Media de notas:\", mean_nota)\n",
        "print(\"Datos ordenados:\", todos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1alhheIvu2t",
        "outputId": "d195a814-65ab-4b2a-8706-00725533d885"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total personas: 6\n",
            "Media de notas: 5.819999999999999\n",
            "Datos ordenados: [('Carla', 21, 4.9), ('Ana', 23, 5.8), ('Nuevo', 25, 6.0), ('Sofía', 26, 6.8), ('Pedro', 28, 5.5), ('Luis', 30, 6.1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Leccion 4\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Leccion4_SQL_DF\").getOrCreate()"
      ],
      "metadata": {
        "id": "pSoy_8WbwCvm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crear datos\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Datos de ejemplo (id, nombre, edad, nota, país)\n",
        "data = [\n",
        "    (1, \"Ana\", 23, 5.8, \"CL\"),\n",
        "    (2, \"Luis\", 30, 6.1, \"AR\"),\n",
        "    (3, \"Carla\", 21, 4.9, \"CL\"),\n",
        "    (4, \"Pedro\", 28, 5.5, \"PE\"),\n",
        "    (5, \"Sofia\", 26, 6.8, \"CL\"),\n",
        "]\n",
        "cols = [\"id\",\"nombre\",\"edad\",\"nota\",\"pais\"]\n",
        "df = spark.createDataFrame(data, cols)"
      ],
      "metadata": {
        "id": "HmTR5QbbwHsu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar en CSV, JSON, Parquet\n",
        "df.write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"/content/df_csv\")\n",
        "df.write.mode(\"overwrite\").json(\"/content/df_json\")\n",
        "df.write.mode(\"overwrite\").parquet(\"/content/df_parquet\")\n",
        "\n",
        "# Cargar desde CSV / JSON / Parquet\n",
        "df_csv = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/content/df_csv\")\n",
        "df_json = spark.read.json(\"/content/df_json\")\n",
        "df_parq = spark.read.parquet(\"/content/df_parquet\")"
      ],
      "metadata": {
        "id": "m1enKtKkwOw2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Consultas\n",
        "df_parq.createOrReplaceTempView(\"personas\")\n",
        "\n",
        "# 2.1 Selección y filtro\n",
        "res1 = spark.sql(\"\"\"\n",
        "  SELECT id, nombre, edad, nota, pais\n",
        "  FROM personas\n",
        "  WHERE pais = 'CL' AND nota >= 5.5\n",
        "  ORDER BY nota DESC\n",
        "\"\"\")\n",
        "res1.show()\n",
        "\n",
        "# 2.2 Agregación por país\n",
        "res2 = spark.sql(\"\"\"\n",
        "  SELECT pais, COUNT(*) AS n, ROUND(AVG(nota),2) AS nota_prom\n",
        "  FROM personas\n",
        "  GROUP BY pais\n",
        "  ORDER BY n DESC\n",
        "\"\"\")\n",
        "res2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPeXIKL8wXnX",
        "outputId": "5916394c-12b6-4b47-96b7-5f870e1e187b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+----+----+----+\n",
            "| id|nombre|edad|nota|pais|\n",
            "+---+------+----+----+----+\n",
            "|  5| Sofia|  26| 6.8|  CL|\n",
            "|  1|   Ana|  23| 5.8|  CL|\n",
            "+---+------+----+----+----+\n",
            "\n",
            "+----+---+---------+\n",
            "|pais|  n|nota_prom|\n",
            "+----+---+---------+\n",
            "|  CL|  3|     5.83|\n",
            "|  PE|  1|      5.5|\n",
            "|  AR|  1|      6.1|\n",
            "+----+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Funciones\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "def categoria_nota(x):\n",
        "    if x is None: return \"desconocido\"\n",
        "    return \"alto\" if x >= 6.0 else (\"medio\" if x >= 5.5 else \"bajo\")\n",
        "\n",
        "udf_cat = udf(categoria_nota, StringType())\n",
        "\n",
        "df_cat = df_parq.withColumn(\"categoria\", udf_cat(F.col(\"nota\")))\n",
        "df_cat.show()\n",
        "\n",
        "df_cat.createOrReplaceTempView(\"personas_cat\")\n",
        "spark.sql(\"\"\"\n",
        "  SELECT nombre, nota, categoria\n",
        "  FROM personas_cat\n",
        "  ORDER BY nota DESC\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfm2q2tWwdbO",
        "outputId": "6d52ea40-8452-4ade-f705-c3d866f31f4e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+----+----+----+---------+\n",
            "| id|nombre|edad|nota|pais|categoria|\n",
            "+---+------+----+----+----+---------+\n",
            "|  3| Carla|  21| 4.9|  CL|     bajo|\n",
            "|  4| Pedro|  28| 5.5|  PE|    medio|\n",
            "|  5| Sofia|  26| 6.8|  CL|     alto|\n",
            "|  1|   Ana|  23| 5.8|  CL|    medio|\n",
            "|  2|  Luis|  30| 6.1|  AR|     alto|\n",
            "+---+------+----+----+----+---------+\n",
            "\n",
            "+------+----+---------+\n",
            "|nombre|nota|categoria|\n",
            "+------+----+---------+\n",
            "| Sofia| 6.8|     alto|\n",
            "|  Luis| 6.1|     alto|\n",
            "|   Ana| 5.8|    medio|\n",
            "| Pedro| 5.5|    medio|\n",
            "| Carla| 4.9|     bajo|\n",
            "+------+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Comparar rendimiento\n",
        "import time\n",
        "\n",
        "# DataFrames (usa Catalyst + Tungsten, más optimizado)\n",
        "t0 = time.time()\n",
        "avg_df = df_parq.select(F.avg(\"nota\")).first()[0]\n",
        "t1 = time.time()\n",
        "\n",
        "# RDD (más manual)\n",
        "rdd = df_parq.rdd\n",
        "t2 = time.time()\n",
        "vals = rdd.map(lambda row: row[\"nota\"]).filter(lambda x: x is not None).collect()\n",
        "avg_rdd = sum(vals)/len(vals)\n",
        "t3 = time.time()\n",
        "\n",
        "print(f\"AVG DataFrame: {avg_df:.3f} | tiempo: {(t1-t0)*1000:.2f} ms\")\n",
        "print(f\"AVG RDD:       {avg_rdd:.3f} | tiempo: {(t3-t2)*1000:.2f} ms\")\n",
        "\n",
        "# (Opcional) ver plan físico optimizado de DataFrames\n",
        "df_parq.select(F.avg(\"nota\")).explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKSMCSF-wi9P",
        "outputId": "b0d42977-e9c8-4b32-bea4-21606e0a2c89"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG DataFrame: 5.820 | tiempo: 421.72 ms\n",
            "AVG RDD:       5.820 | tiempo: 590.46 ms\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[], functions=[avg(nota#73)])\n",
            "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=262]\n",
            "      +- HashAggregate(keys=[], functions=[partial_avg(nota#73)])\n",
            "         +- FileScan parquet [nota#73] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/df_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<nota:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Leccion 5\n",
        "#Creacion directorio y session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"L5_Streaming\").getOrCreate()\n",
        "\n",
        "import shutil, os, time\n",
        "base_dir = \"/content/stream_in\"\n",
        "shutil.rmtree(base_dir, ignore_errors=True)\n",
        "os.makedirs(base_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "0wtlmxgswpEH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Esquema de eventos\n",
        "from pyspark.sql import functions as F, types as T\n",
        "\n",
        "schema = T.StructType([\n",
        "    T.StructField(\"event_time\", T.TimestampType(), True),\n",
        "    T.StructField(\"source\", T.StringType(), True),\n",
        "    T.StructField(\"value\", T.DoubleType(), True),\n",
        "])\n",
        "\n",
        "# Lee archivos JSON que vayan apareciendo en la carpeta\n",
        "events = (spark.readStream\n",
        "          .schema(schema)\n",
        "          .option(\"maxFilesPerTrigger\", 1)   # 1 archivo por microbatch\n",
        "          .json(base_dir))"
      ],
      "metadata": {
        "id": "19PvcTJoxERh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformaciones\n",
        "agg = (events\n",
        "       .withWatermark(\"event_time\", \"10 minutes\")\n",
        "       .groupBy(F.window(\"event_time\", \"5 minutes\", \"2 minutes\"), \"source\")\n",
        "       .agg(F.count(\"*\").alias(\"n\"), F.avg(\"value\").alias(\"avg_value\"))\n",
        "       .orderBy(F.col(\"window\").asc(), F.col(\"source\").asc()))  # OK si usamos complete\n",
        "\n",
        "query = (agg.writeStream\n",
        "         .outputMode(\"complete\")      # << antes estaba \"update\"\n",
        "         .format(\"console\")\n",
        "         .option(\"truncate\", \"false\")\n",
        "         .start())"
      ],
      "metadata": {
        "id": "4sFmwXeWxFm8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generar datos\n",
        "# Generamos tres micro-lotes con datos sintéticos\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "t0 = datetime.now().replace(second=0, microsecond=0)\n",
        "\n",
        "def write_batch(idx, n=200):\n",
        "    rows = []\n",
        "    for i in range(n):\n",
        "        ts = t0 + timedelta(seconds=idx*30 + i)  # tiempos crecientes\n",
        "        rows.append({\"event_time\": ts.isoformat(sep=\" \"),\n",
        "                     \"source\": \"sensor_A\" if i%2==0 else \"sensor_B\",\n",
        "                     \"value\": 50 + i%7})\n",
        "    pd.DataFrame(rows).to_json(f\"{base_dir}/batch_{idx:03d}.json\",\n",
        "                               orient=\"records\", lines=True)\n",
        "\n",
        "for i in range(3):      # tres microbatches\n",
        "    write_batch(i, n=300)\n",
        "    time.sleep(2)       # simula llegada gradual de archivos\n"
      ],
      "metadata": {
        "id": "-D8C6AfTxlkq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Último progreso del stream (muestra micro-lotes, filas procesadas, etc.)\n",
        "print(query.lastProgress)        # dict o None si aún no hay lotes\n",
        "print(query.status)              # estado general"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb_eAqzcx8qE",
        "outputId": "6da8b9ee-7aaa-496f-c5e7-dd7b7b7ee5fb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '8babd850-e891-4a6f-b021-f5af9a18936f', 'runId': '6d943d1c-f61c-43ad-9b52-e6d42a96b6e5', 'name': None, 'timestamp': '2025-09-13T20:32:48.176Z', 'batchId': 3, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'latestOffset': 2, 'triggerExecution': 2}, 'eventTime': {'watermark': '2025-09-13T20:26:59.000Z'}, 'stateOperators': [{'operatorName': 'stateStoreSave', 'numRowsTotal': 20, 'numRowsUpdated': 0, 'allUpdatesTimeMs': 218, 'numRowsRemoved': 0, 'allRemovalsTimeMs': 0, 'commitTimeMs': 628, 'memoryUsedBytes': 7664, 'numRowsDroppedByWatermark': 0, 'numShufflePartitions': 4, 'numStateStoreInstances': 4, 'customMetrics': {'loadedMapCacheHitCount': 28, 'loadedMapCacheMissCount': 0, 'stateOnCurrentVersionSizeBytes': 5408}}], 'sources': [{'description': 'FileStreamSource[file:/content/stream_in]', 'startOffset': {'logOffset': 2}, 'endOffset': {'logOffset': 2}, 'latestOffset': None, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0}], 'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@228dbd89', 'numOutputRows': 0}}\n",
            "{'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parar datos\n",
        "\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "dos1wkJ5xp5L"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparación simple con batch (mismo esquema y datos combinados)\n",
        "df_batch = spark.read.schema(schema).json(base_dir)   # lee todo junto (batch)\n",
        "\n",
        "import time\n",
        "t0 = time.time()\n",
        "res = (df_batch\n",
        "       .groupBy(F.window(\"event_time\",\"5 minutes\",\"2 minutes\"), \"source\")\n",
        "       .agg(F.count(\"*\").alias(\"n\"), F.avg(\"value\").alias(\"avg_value\"))\n",
        "       .orderBy(\"window\",\"source\")\n",
        "       ).collect()\n",
        "t1 = time.time()\n",
        "print(f\"Batch: {len(res)} filas agregadas en {1000*(t1-t0):.2f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE5ng5bfyIvl",
        "outputId": "986692f7-d92c-401a-e936-f92e15a72b76"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: 10 filas agregadas en 1167.65 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Leccion 6\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Leccion6_MLlib\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "dUrr0eKdyeQW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # selecciona wdbc.data\n",
        "#link https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "0MFLoTjcyq9_",
        "outputId": "ee4b49f2-4902-45a3-d155-0f97890ae617"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9ceb9d89-3389-49d7-922e-b0dcdc0c4c34\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9ceb9d89-3389-49d7-922e-b0dcdc0c4c34\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving wdbc.data to wdbc.data\n",
            "Saving wdbc.names to wdbc.names\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar WDBC (desde archivo ya subido o vía wget)\n",
        "df_wdbc = spark.read.csv(\"wdbc.data\", header=False, inferSchema=True)\n",
        "\n",
        "# 3) Verifica cuántas columnas tiene (debe ser 32)\n",
        "print(\"n_cols:\", len(df_wdbc.columns))\n",
        "df_wdbc.show(3, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s3DnCRsyz23",
        "outputId": "6243d60c-b929-490d-e4b2-0dada3cd144a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_cols: 32\n",
            "+--------+---+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+\n",
            "|_c0     |_c1|_c2  |_c3  |_c4  |_c5   |_c6    |_c7    |_c8   |_c9    |_c10  |_c11   |_c12  |_c13  |_c14 |_c15 |_c16    |_c17   |_c18   |_c19   |_c20   |_c21    |_c22 |_c23 |_c24 |_c25  |_c26  |_c27  |_c28  |_c29  |_c30  |_c31   |\n",
            "+--------+---+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+\n",
            "|842302  |M  |17.99|10.38|122.8|1001.0|0.1184 |0.2776 |0.3001|0.1471 |0.2419|0.07871|1.095 |0.9053|8.589|153.4|0.006399|0.04904|0.05373|0.01587|0.03003|0.006193|25.38|17.33|184.6|2019.0|0.1622|0.6656|0.7119|0.2654|0.4601|0.1189 |\n",
            "|842517  |M  |20.57|17.77|132.9|1326.0|0.08474|0.07864|0.0869|0.07017|0.1812|0.05667|0.5435|0.7339|3.398|74.08|0.005225|0.01308|0.0186 |0.0134 |0.01389|0.003532|24.99|23.41|158.8|1956.0|0.1238|0.1866|0.2416|0.186 |0.275 |0.08902|\n",
            "|84300903|M  |19.69|21.25|130.0|1203.0|0.1096 |0.1599 |0.1974|0.1279 |0.2069|0.05999|0.7456|0.7869|4.585|94.03|0.00615 |0.04006|0.03832|0.02058|0.0225 |0.004571|23.57|25.53|152.5|1709.0|0.1444|0.4245|0.4504|0.243 |0.3613|0.08758|\n",
            "+--------+---+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generar target\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Renombrar columnas (ID, Diagnosis, 30 features)\n",
        "cols = [\"ID\",\"Diagnosis\"] + [f\"feat_{i}\" for i in range(30)]\n",
        "assert len(cols) == len(df_wdbc.columns), \"El CSV no tiene 32 columnas\"\n",
        "df_wdbc = df_wdbc.toDF(*cols)\n",
        "\n",
        "# label binaria: M=1, B=0\n",
        "df_wdbc = df_wdbc.withColumn(\"label\", F.when(F.col(\"Diagnosis\")==\"M\", 1).otherwise(0))\n",
        "df_wdbc.select(\"ID\",\"Diagnosis\",\"label\").show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srM0gvurzilL",
        "outputId": "bca856a3-207b-4bbb-df21-21a652aef157"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+-----+\n",
            "|ID      |Diagnosis|label|\n",
            "+--------+---------+-----+\n",
            "|842302  |M        |1    |\n",
            "|842517  |M        |1    |\n",
            "|84300903|M        |1    |\n",
            "|84348301|M        |1    |\n",
            "|84358402|M        |1    |\n",
            "+--------+---------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ensamblar\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "\n",
        "feature_cols = [f\"feat_{i}\" for i in range(30)]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
        "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withMean=True, withStd=True)\n",
        "\n",
        "df_assembled = assembler.transform(df_wdbc)\n",
        "df_scaled = scaler.fit(df_assembled).transform(df_assembled).select(\"label\",\"features\")\n",
        "df_scaled.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LigwP6Sz4M8",
        "outputId": "d41cb48d-6ef8-4972-b139-a75344d9c4df"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|label|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
            "+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1    |[1.096099529431716,-2.0715123022444226,1.2688172627037961,0.9835095201041407,1.5670874574786495,3.2806280641246874,2.6505417863835716,2.530248864134294,2.2155655418463147,2.2537638107280564,2.4875451492360856,-0.5647681226567445,2.83054028878839,2.485390675169629,-0.21381351331683426,1.3157038876724816,0.7233896514447252,0.6602390007084605,1.1477467710198095,0.9062856451675914,1.8850309966167536,-1.3580984902594602,2.3015754798257215,1.9994781593288942,1.3065366565450884,2.6143646582121103,2.1076718175574505,2.2940575987078202,2.748204114212146,1.9353117420601103]                   |\n",
            "|1    |[1.8282119737343636,-0.353321522550086,1.684472552277105,1.9070302686337905,-0.826235446757048,-0.48664347761613375,-0.02382489180553116,0.5476622708254755,0.0013911392435855005,-0.86788880680381,0.4988156954477679,-0.8754732788647083,0.263095469564322,0.7417492871622354,-0.6048186703081381,-0.6923171035200322,-0.4403925587579929,0.25993335350933555,-0.8047422918987197,-0.09935631723496176,1.804339809655201,-0.3688786474058127,1.5337764307706196,1.8888270198661696,-0.37528174828342464,-0.4300658061196981,-0.14661995820849835,1.086128616484048,-0.2436752590262775,0.28094278650452165]|\n",
            "|1    |[1.578499202034236,0.45578590821260595,1.5651259839837786,1.5575131853441078,0.9413821230379442,1.0519998953324945,1.3622797889632112,2.0354397832616917,0.9388587199172286,-0.3976580132373034,1.2275957908432695,-0.7793975887887334,0.8501802317332622,1.1802975182609179,-0.29674390862418915,0.8142570439319705,0.21288911460373786,1.423574870468736,0.2368271524462948,0.2933013298662232,1.5105411289233945,-0.023953307463422467,1.3462906164664663,1.4550042984805673,0.5269437501402047,1.0819801384826762,0.8542223185933945,1.9532816641761763,1.1512420284425282,0.20121416214039706]          |\n",
            "|1    |[-0.7682333229203733,0.25350905052193273,-0.5921661228907591,-0.7637917361139567,3.2806668392992155,3.3999174223523063,1.9142128745181857,1.4504311303550208,2.8648621541416777,4.9066019925053626,0.32608651895373264,-0.11031198110869007,0.2863414542500339,-0.28812462897677593,0.6890953287644468,2.7418678508275645,0.8187979282845723,1.114026778909898,4.7285197736461075,2.045710867506448,-0.2812170225818496,0.13386630562342564,-0.24971957736396574,-0.5495376930878059,3.391290720863764,3.8899746669624107,1.9878391694729327,2.1738732289399634,6.040726147322177,4.930671865740074]         |\n",
            "|1    |[1.7487579100115953,-1.1508038465489459,1.7750113282237656,1.824623801841914,0.2801253491403808,0.5388663067660681,1.3698061492207791,1.4272369546891175,-0.009552062087235042,-0.5619555194231912,1.2694258210589437,-0.7895489824571383,1.2720701240079932,1.1893102889251013,1.4817633642323367,-0.04847722508705989,0.827742454238308,1.1431988503657047,-0.36077482839741853,0.4988891643446104,1.2974336351531401,-1.4654809074113357,1.3373627205472205,1.2196510812106454,0.2203622700933402,-0.3131189991391025,0.6126397000550066,0.7286181494530828,-0.8675895961791853,-0.3967505205905387]      |\n",
            "+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#entrenar modelo\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Split\n",
        "train, test = df_scaled.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Modelo\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Grid de hiperparámetros\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.0, 0.01, 0.1])\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "             .build())\n",
        "\n",
        "# Evaluador\n",
        "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\", labelCol=\"label\")\n",
        "\n",
        "# Pipeline (solo el modelo porque ya escalamos antes)\n",
        "pipeline = Pipeline(stages=[lr])\n",
        "\n",
        "# Cross-Validation\n",
        "cv = CrossValidator(estimator=pipeline,\n",
        "                    estimatorParamMaps=paramGrid,\n",
        "                    evaluator=evaluator,\n",
        "                    numFolds=3,\n",
        "                    parallelism=2)\n",
        "\n",
        "cv_model = cv.fit(train)\n",
        "pred = cv_model.transform(test)"
      ],
      "metadata": {
        "id": "59U22jhQz8us"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Metricas del modelo\n",
        "# AUC-ROC\n",
        "auc = evaluator.evaluate(pred)\n",
        "print(f\"AUC-ROC: {auc:.4f}\")\n",
        "\n",
        "# Matriz de confusión y métricas\n",
        "tp = pred.filter(\"label = 1 AND prediction = 1\").count()\n",
        "fp = pred.filter(\"label = 0 AND prediction = 1\").count()\n",
        "tn = pred.filter(\"label = 0 AND prediction = 0\").count()\n",
        "fn = pred.filter(\"label = 1 AND prediction = 0\").count()\n",
        "\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "accuracy  = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "print(f\"Confusion Matrix -> TN:{tn} FP:{fp} FN:{fn} TP:{tp}\")\n",
        "print(f\"Precisión: {precision:.4f} | Recall: {recall:.4f} | Exactitud: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wALSvMNa0ucw",
        "outputId": "f96baeb0-7cb1-4fa9-aabc-f1009fd7759e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC-ROC: 0.9936\n",
            "Confusion Matrix -> TN:54 FP:0 FN:2 TP:30\n",
            "Precisión: 1.0000 | Recall: 0.9375 | Exactitud: 0.9767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Guardar modelo\n",
        "save_path = \"/content/modelo_wdbc_lr_cv\"\n",
        "cv_model.bestModel.write().overwrite().save(save_path)\n",
        "print(\"Modelo guardado en:\", save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd6NkAUn04mY",
        "outputId": "7d6514c4-c8cb-4633-ce44-fd4c0c3e11d9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo guardado en: /content/modelo_wdbc_lr_cv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cargar y aplicar modelo\n",
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "modelo = PipelineModel.load(save_path)\n",
        "pred2 = modelo.transform(test)\n",
        "pred2.select(\"features\",\"label\",\"probability\",\"prediction\").show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCefXSEg0_LI",
        "outputId": "2d902ba3-3120-4f13-e2e1-c56beee0682e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+------------------------------------------+----------+\n",
            "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |label|probability                               |prediction|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+------------------------------------------+----------+\n",
            "|[-1.8156081414944316,1.4415949617855404,-1.8103241992640196,-1.352855893560715,-1.093582271472622,-1.0520371659775176,-1.1138927357766057,-1.2607102919261142,0.21295969830611972,1.4237781329269337,-0.09906525366937285,0.4443891729148628,-0.18500797990919102,-0.4659619818383469,1.8714363060379795,-0.8815043982408396,-1.0565710133486719,-1.91176529931183,1.0013705773313692,1.1628925194521962,-1.488067518855532,0.8530030992768995,-1.4906971101390734,-1.1110483166788065,-0.29644651055708826,-1.0859848539673862,-1.3046826692972786,-1.7435286970376342,0.2541631239869231,0.8545459451241966]      |0    |[0.9995001189996321,4.998810003679077E-4] |0.0       |\n",
            "|[-1.5766784212995368,-1.439106494291974,-1.5407655708773689,-1.2320877956760683,0.5147648495556449,-0.5305722571539239,-0.792140834765581,-0.8715635668648485,-0.4874051868664072,1.1985772408273125,-1.0041805626464675,-0.974449367131656,-0.8888172609255364,-0.7272444016297726,0.2851010737129549,-0.4388016613616892,-0.45993800436060034,-0.7254343934423574,-0.5591690082726593,0.049165776423137866,-1.4061349290176477,-1.175873782365367,-1.3082704368558262,-1.0625725793903749,1.3897516297006651,-0.19553661168578623,-0.4639368103363016,-0.44858014581738914,0.13293624500643578,1.1601723385200078]|0    |[0.9994937966484596,5.062033515403908E-4] |0.0       |\n",
            "|[-1.5690167794405554,0.39301033168791444,-1.5354155522987027,-1.2306669945244844,1.9865944430695759,-0.27854981937461976,-0.737574722898214,-1.0220937669365604,0.05975487967462907,0.6759412082187563,-0.16938297687675657,1.5411022388764626,-0.18451338449034613,-0.4824487574435605,1.6283070346549715,0.3429640193028045,-0.06273479626693153,-0.411186357037862,1.5759878666041711,-0.2822690229105859,-1.387100084913897,0.221724646929506,-1.3451724066553736,-1.0655584037885755,1.3809921588421832,-0.5368433255367635,-0.8732372031337805,-1.3209056853729444,0.1151563027559645,-0.3779257065045652]    |0    |[0.9978348263177799,0.002165173682220134] |0.0       |\n",
            "|[-1.4895627157177875,-0.8834263909808149,-1.449815255040041,-1.1761082303036559,-0.9549316575908743,-0.5180752767681731,-0.5216936228436433,-0.6470954890314959,0.4318237249225346,0.48331654579392236,0.47934309517495405,-0.030551030856937313,0.6978448427290216,-0.2232766449296023,1.295253238239892,0.196103752149403,0.008158853884899392,0.5289646791573307,0.9469331499265776,0.1432675609546057,-1.301029485488241,-1.2978992564015899,-1.2496439203194483,-1.016204482853614,-1.3519627490041486,-0.8228545382610463,-0.8521466570709055,-1.0190755719306317,-0.7544445091307305,-0.5335072582151138]    |0    |[0.9997637498705122,2.3625012948780277E-4]|0.0       |\n",
            "|[-1.4058521842955858,-1.262404871481731,-1.348576441936047,-1.1195603444706097,-1.3616401249773327,-0.318691635159152,-0.3627619820713359,-0.6988958146853466,1.9310423072449754,0.9677109174799005,0.016688536841243717,1.9000265150093496,-0.12763491132317825,-0.36967921230389933,0.5651993164980369,0.7762855680139807,0.37190290933681625,0.6164808935247506,2.5861045751153053,0.7660777654038382,-1.2950293715859718,-1.0489672893676951,-1.2401208313389198,-1.0019779077798352,-1.4894864414823137,-0.549554934991176,-0.6350578318009931,-0.9696326350008172,0.6162274025419777,0.052830333462720355]    |0    |[0.9998952633767568,1.0473662324317345E-4]|0.0       |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+------------------------------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}